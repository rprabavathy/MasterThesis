{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.patches as mpatches\n",
    "from skimage.metrics import hausdorff_distance\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from skimage.morphology import remove_small_objects, binary_erosion\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6283ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary(mask):\n",
    "    \"\"\"Extracts the boundary of a binary mask.\"\"\"\n",
    "    mask = mask.astype(bool)\n",
    "    return mask ^ binary_erosion(mask)\n",
    "\n",
    "def get_points(mask):\n",
    "    \"\"\"Returns the coordinates of nonzero points in the mask.\"\"\"\n",
    "    return np.argwhere(mask)\n",
    "\n",
    "def calHDPercentile(mask1, mask2, filename=\"hd_percentiles.dat\"):\n",
    "    # Get boundaries\n",
    "    boundary1 = get_boundary(mask1)\n",
    "    boundary2 = get_boundary(mask2)\n",
    "    points1 = get_points(boundary1)\n",
    "    points2 = get_points(boundary2)\n",
    "\n",
    "    percentiles = np.arange(0, 101, 5)\n",
    "    if points1.size == 0 or points2.size == 0:\n",
    "        np.savetxt(filename, np.column_stack((percentiles, np.full_like(percentiles, np.nan, dtype=float))),\n",
    "                   fmt=\"%.1f %.6f\", header=\"percentile hd\")\n",
    "        return\n",
    "\n",
    "    tree2 = cKDTree(points2)\n",
    "    dists1, _ = tree2.query(points1, k=1)\n",
    "    tree1 = cKDTree(points1)\n",
    "    dists2, _ = tree1.query(points2, k=1)\n",
    "\n",
    "    hd_values = [max(np.percentile(dists1, p), np.percentile(dists2, p)) for p in percentiles]\n",
    "    np.savetxt(filename, np.column_stack((percentiles, hd_values)), fmt=\"%.1f %.6f\", header=\"percentile hd\")\n",
    "\n",
    "# Example usage for multi-class masks:\n",
    "def saveHDPercentile(gt_mask, pred_mask, class_labels, prefix=\"HD\"):\n",
    "    \"\"\"\n",
    "    gt_mask: ground truth mask (2D numpy array, integer class labels)\n",
    "    pred_mask: predicted mask (2D numpy array, integer class labels)\n",
    "    class_labels: list or array of class label values, e.g. [0, 100, 255]\n",
    "    prefix: prefix for output files\n",
    "    \"\"\"\n",
    "    for cls in class_labels:\n",
    "        gt_bin = (gt_mask == cls).astype(np.uint8)\n",
    "        pred_bin = (pred_mask == cls).astype(np.uint8)\n",
    "        filename = f\"{prefix}_class{cls}.dat\"\n",
    "        calHDPercentile(gt_bin, pred_bin, filename)\n",
    "        print(f\"Saved HD percentiles for class {cls} to {filename}\")\n",
    "    \n",
    "     # Overall (all foreground as one class, i.e., not background)\n",
    "    # If background is 0, foreground is everything else\n",
    "    gt_foreground = (gt_mask != 0).astype(np.uint8)\n",
    "    pred_foreground = (pred_mask != 0).astype(np.uint8)\n",
    "    filename = f\"{prefix}_overall.dat\"\n",
    "    calHDPercentile(gt_foreground, pred_foreground, filename)\n",
    "    print(f\"Saved HD percentiles for overall (all foreground) to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381289d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hd95(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Computes the 95th percentile Hausdorff Distance (HD95) between two binary masks.\n",
    "    Parameters:\n",
    "        mask1, mask2: 2D numpy arrays (binary masks)\n",
    "    Returns:\n",
    "        hd95: float (the HD95 value)\n",
    "    \"\"\"\n",
    "    # Get boundaries\n",
    "    boundary1 = get_boundary(mask1)\n",
    "    boundary2 = get_boundary(mask2)\n",
    "    points1 = get_points(boundary1)\n",
    "    points2 = get_points(boundary2)\n",
    "\n",
    "    # Handle empty boundaries\n",
    "    if points1.size == 0 or points2.size == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Compute distances from boundary1 to boundary2\n",
    "    tree2 = cKDTree(points2)\n",
    "    dists1, _ = tree2.query(points1, k=1)\n",
    "    # Compute distances from boundary2 to boundary1\n",
    "    tree1 = cKDTree(points1)\n",
    "    dists2, _ = tree1.query(points2, k=1)\n",
    "\n",
    "    # Take the 95th percentile in both directions\n",
    "    hd95_1 = np.percentile(dists1, 95)\n",
    "    hd95_2 = np.percentile(dists2, 95)\n",
    "    return max(hd95_1, hd95_2)\n",
    "\n",
    "def hausdorffDistance(gt_mask, pred_mask):\n",
    "    \"\"\"Compute the Hausdorff Distance between the boundaries of two binary masks.\"\"\"\n",
    "    # Extract contours\n",
    "    \n",
    "    gt_mask = (gt_mask > 0).astype(np.uint8)\n",
    "    pred_mask = (pred_mask > 0).astype(np.uint8)\n",
    "    # Remove small objects in the contour (optional)\n",
    "    # gt_contour = remove_small_objects(gt_mask.astype(bool), min_size=1000)\n",
    "    # pred_contour = remove_small_objects(pred_mask.astype(bool), min_size=1000)\n",
    "    gt_contour,  pred_contour =  gt_mask, pred_mask\n",
    "    # Compute Hausdorff on contours\n",
    "    return hausdorff_distance(gt_contour, pred_contour)\n",
    "\n",
    "def calculate_metrics(gt_mask, pred_mask, class_labels, ignore_labels=(200, 150), min_size=1000):\n",
    "    \"\"\"Calculate metrics (IoU, Precision, Recall, F1, Hausdorff, HD95) for each class.\"\"\"\n",
    "    metrics = {}\n",
    "    # valid_mask = (gt_mask != ignore_label)\n",
    "    ignore_labels = set(ignore_labels)\n",
    "    valid_mask = ~np.isin(gt_mask, list(ignore_labels))\n",
    "    img_diag = np.sqrt(gt_mask.shape[0]**2 + gt_mask.shape[1]**2)\n",
    "\n",
    "    for class_label in class_labels:\n",
    "        # Mask out ignore regions\n",
    "        gt_class_mask = ((gt_mask == class_label) & valid_mask)\n",
    "        pred_class_mask = ((pred_mask == class_label) & valid_mask)\n",
    "        # Convert to uint8 for further processing\n",
    "        gt_class_mask = gt_class_mask.astype(np.uint8)\n",
    "        pred_class_mask = pred_class_mask.astype(np.uint8)\n",
    "\n",
    "        # Metrics\n",
    "        tp = np.sum((gt_class_mask == 1) & (pred_class_mask == 1))\n",
    "        fp = np.sum((gt_class_mask == 0) & (pred_class_mask == 1))\n",
    "        fn = np.sum((gt_class_mask == 1) & (pred_class_mask == 0))\n",
    "        tn = np.sum((gt_class_mask == 0) & (pred_class_mask == 0))\n",
    "        iou = tp / (tp + fp + fn) if (tp + fp + fn) != 0 else 0.0\n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n",
    "        f1_s = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0.0\n",
    "        accuracy = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) != 0 else 0.0\n",
    "\n",
    "        # Hausdorff distances\n",
    "        hd = hausdorffDistance(gt_class_mask, pred_class_mask)\n",
    "        hd95_val = hd95(gt_class_mask, pred_class_mask)\n",
    "        # hd95_val = hd95_boundary(gt_class_mask, pred_class_mask, min_size=min_size)\n",
    "\n",
    "        metrics[class_label] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'IoU (Jaccard)': iou,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1_s,\n",
    "            'Hausdorff Distance': hd,\n",
    "            'HD95': hd95_val,\n",
    "            'HD/Diag (%)': 100 * hd / img_diag if img_diag > 0 else np.nan,\n",
    "            # 'HD95/Diag (%)': 100 * hd95_val / img_diag if img_diag > 0 else np.nan\n",
    "        }\n",
    "        \n",
    "    # Overall metrics (excluding ignore label)\n",
    "    gt_valid = gt_mask[valid_mask].flatten()\n",
    "    pred_valid = pred_mask[valid_mask].flatten()    \n",
    "    \n",
    "    class_labels = [0, 100, 255]\n",
    "    # Compute multiclass metrics    \n",
    "    overall_accuracy = accuracy_score(gt_valid, pred_valid)\n",
    "    overall_precision = precision_score(gt_valid, pred_valid, average='macro', labels=class_labels, zero_division=0)\n",
    "    overall_recall = recall_score(gt_valid, pred_valid, average='macro', labels=class_labels, zero_division=0)\n",
    "    overall_f1 = f1_score(gt_valid, pred_valid, average='macro', labels=class_labels, zero_division=0)\n",
    "    overall_iou = jaccard_score(gt_valid, pred_valid, average='macro', labels=class_labels, zero_division=0)\n",
    "\n",
    "    overall_hd = np.mean([hausdorffDistance((gt_mask == c) & valid_mask, (pred_mask == c) & valid_mask) for c in class_labels])\n",
    "    overall_hd95 = np.mean([hd95((gt_mask == c) & valid_mask, (pred_mask == c) & valid_mask) for c in class_labels])\n",
    "\n",
    "    metrics['overall'] = {\n",
    "        'Accuracy': overall_accuracy,\n",
    "        'IoU (Jaccard)': overall_iou,\n",
    "        'Precision': overall_precision,\n",
    "        'Recall': overall_recall,\n",
    "        'F1 Score': overall_f1,\n",
    "        'Hausdorff Distance': overall_hd,\n",
    "        'HD95': overall_hd95,\n",
    "        'HD/Diag (%)': 100 * overall_hd95 / img_diag if img_diag > 0 else np.nan,\n",
    "        }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9bda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_coords(mask):\n",
    "    # Returns a list of (x, y) coordinates where mask is True\n",
    "    return np.column_stack(np.where(mask))\n",
    "\n",
    "def plot_boundaries(gt_mask, pred_mask, class_label, ignore_labels=(200,150), min_size=1000, thickness=10, output_dir=None):\n",
    "    \"\"\"Visualize boundaries of GT and prediction for a given class.\"\"\"\n",
    "    # valid_mask = (gt_mask != ignore_label)\n",
    "    ignore_labels = set(ignore_labels)\n",
    "    valid_mask = ~np.isin(gt_mask, list(ignore_labels))\n",
    "    \n",
    "    gt_class = ((gt_mask == class_label) & valid_mask)\n",
    "    pred_class = ((pred_mask == class_label) & valid_mask)\n",
    "    gt_class = remove_small_objects(gt_class, min_size=min_size)\n",
    "    pred_class = remove_small_objects(pred_class, min_size=min_size)    \n",
    "    gt_contour = gt_class ^ binary_erosion(gt_class)\n",
    "    pred_contour = pred_class ^ binary_erosion(pred_class)\n",
    "    \n",
    "    gt_coords = mask_to_coords(gt_contour)\n",
    "    pred_coords = mask_to_coords(pred_contour)\n",
    "    \n",
    "    # Save to CSV or TXT\n",
    "    gt_file = os.path.join(output_dir, f'gt_contour_{class_label}.txt')\n",
    "    pred_file = os.path.join(output_dir, f'pred_contour_{class_label}.txt')\n",
    "    np.savetxt(gt_file, gt_coords, fmt='%d', delimiter=' ')\n",
    "    np.savetxt(pred_file, pred_coords, fmt='%d', delimiter=' ')\n",
    "    \n",
    "    from skimage.morphology import dilation, disk\n",
    "    gt_contour = dilation(gt_contour, disk(thickness))\n",
    "    pred_contour = dilation(pred_contour, disk(thickness))\n",
    "\n",
    "    # Light gray background\n",
    "    # colors = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]  # Matplotlib uses [0,1] for RGB\n",
    "    # overlay = np.ones((gt_class.shape[0], gt_class.shape[1], 4)) * 0.9\n",
    "    # overlay[gt_contour, :3] = colors[0]\n",
    "    # overlay[gt_contour, 3] = 1  # 50% alpha\n",
    "    # overlay[pred_contour, :3] = colors[2]\n",
    "    # overlay[pred_contour, 3] = 1  # 50% \n",
    "    overlay = np.ones((*gt_class.shape, 3), dtype=np.uint8) * 255\n",
    "    overlay[gt_contour, 0] = 255  # Red channel for GT\n",
    "    overlay[gt_contour, 1] = 0\n",
    "    overlay[gt_contour, 2] = 0\n",
    "    overlay[pred_contour, 0] = 0\n",
    "    overlay[pred_contour, 1] = 0\n",
    "    overlay[pred_contour, 2] = 150  # Blue channel for Prediction\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(overlay, alpha=1, cmap='gray')\n",
    "    red_patch = mpatches.Patch(color='red', label='Ground Truth')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Predicted')\n",
    "    plt.legend(handles=[red_patch, blue_patch], loc='lower left', frameon=True, prop={'weight':'bold', 'size':12})\n",
    "    plt.axis('off')\n",
    "    BC_file = os.path.join(output_dir, f'BC_{class_label}.png')\n",
    "    plt.savefig(BC_file, bbox_inches='tight', dpi=300)\n",
    "    # plt.title(f\"Boundaries for class {class_label}\")\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def compute_and_plot_confusion_matrices(gt_mask, pred_mask, class_labels, ignore_label=(200,150)):\n",
    "    gt = gt_mask.flatten()\n",
    "    pred = pred_mask.flatten()\n",
    "    # if ignore_label is not None:\n",
    "    #     valid = gt != ignore_label\n",
    "    #     gt = gt[valid]\n",
    "    #     pred = pred[valid]\n",
    "    if ignore_label is not None:\n",
    "        # Use np.isin for multiple ignore labels\n",
    "        valid = ~np.isin(gt, ignore_label)\n",
    "        gt = gt[valid]\n",
    "        pred = pred[valid]\n",
    "\n",
    "    # Overall multiclass confusion matrix\n",
    "    cm = confusion_matrix(gt, pred, labels=class_labels)\n",
    "    print(\"Overall Confusion Matrix (rows: True, columns: Predicted):\\n\", cm)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.savefig(\"cm_overall.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.title('Overall Confusion Matrix')\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Per-class confusion matrices (standard layout)\n",
    "    mcm = multilabel_confusion_matrix(gt, pred, labels=class_labels)\n",
    "    for idx, label in enumerate(class_labels):\n",
    "        tn, fp, fn, tp = mcm[idx].ravel()\n",
    "        mat = np.array([[tp, fp],\n",
    "                        [fn, tn]])\n",
    "        print(f\"\\nConfusion Matrix for class {label} (One-vs-Rest):\")\n",
    "        print(mat)\n",
    "        plt.figure()\n",
    "        cell_labels = np.array([['TP', 'FP'],\n",
    "                                ['FN', 'TN']])\n",
    "        annot = np.empty_like(mat, dtype=object)\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                annot[i, j] = f\"{cell_labels[i, j]}\\n{mat[i, j]}\"\n",
    "        sns.heatmap(mat, annot=annot, fmt='', cmap='Oranges',\n",
    "                    xticklabels=['Positive', 'Negative'],\n",
    "                    yticklabels=['Positive', 'Negative'])\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.savefig(f\"cm_cls_{label}.png\", bbox_inches='tight', dpi=300)\n",
    "        plt.title(f'One-vs-Rest Confusion Matrix for class {label}')\n",
    "        # plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_class_confusion_scatter(gt_mask, pred_mask, class_labels, ignore_label=(200,150), class_colors=None, output_dir=None): \n",
    "    \"\"\"\n",
    "    For each class, plot a 2x2 confusion matrix (TP, FP, FN, TN) \n",
    "    with actual pixel locations scattered in each cell.\n",
    "    \"\"\"\n",
    "    h, w = gt_mask.shape\n",
    "    gt = gt_mask.flatten()\n",
    "    pred = pred_mask.flatten()\n",
    "    if ignore_label is not None:\n",
    "        valid = ~np.isin(gt, ignore_label)\n",
    "        gt = gt[valid]\n",
    "        pred = pred[valid]\n",
    "        flat_indices = np.arange(h*w)[valid]\n",
    "    else:\n",
    "        flat_indices = np.arange(h*w)\n",
    "    \n",
    "    # Prepare coordinates for all valid pixels\n",
    "    coords = np.column_stack(np.unravel_index(flat_indices, (h, w)))\n",
    "    # Default colors if not provided\n",
    "    if class_colors is None:\n",
    "        cmap = plt.colormaps.get_cmap('tab10')\n",
    "        class_colors = [cmap(i) for i in range(len(class_labels))]\n",
    "    \n",
    "    mcm = multilabel_confusion_matrix(gt, pred, labels=class_labels)\n",
    "    for idx, label in enumerate(class_labels):\n",
    "        # Find indices for TP, FP, FN, TN\n",
    "        true = (gt == label)\n",
    "        pred_ = (pred == label)\n",
    "        TP = np.where(true & pred_)[0]\n",
    "        FP = np.where(~true & pred_)[0]\n",
    "        FN = np.where(true & ~pred_)[0]\n",
    "        TN = np.where(~true & ~pred_)[0]\n",
    "\n",
    "        # Prepare scatter data for each cell\n",
    "        scatter_dict = {\n",
    "            'TP': coords[TP],\n",
    "            'FP': coords[FP],\n",
    "            'FN': coords[FN],\n",
    "            'TN': coords[TN],\n",
    "        }\n",
    "        cell_labels = np.array([['TP', 'FP'], ['FN', 'TN']])\n",
    "        cell_keys = np.array([['TP', 'FP'], ['FN', 'TN']])\n",
    "\n",
    "        fig, axarr = plt.subplots(2, 2, figsize=(8, 8))\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                key = cell_keys[i, j]\n",
    "                ax = axarr[i, j]\n",
    "                ax.set_title(cell_labels[i, j], fontsize=12, fontweight='bold')\n",
    "                ax.set_xlim([0, w])\n",
    "                ax.set_ylim([h, 0])  # image coordinates: y=0 at top\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                ax.spines['bottom'].set_linewidth(2)\n",
    "                ax.spines['left'].set_linewidth(2)\n",
    "                ax.spines['top'].set_linewidth(2)\n",
    "                ax.spines['right'].set_linewidth(2)\n",
    "                if scatter_dict[key].shape[0] > 0:\n",
    "                    ax.scatter(\n",
    "                        scatter_dict[key][:, 1],  # x\n",
    "                        scatter_dict[key][:, 0],  # y\n",
    "                        s=5,\n",
    "                        color=class_colors[idx],\n",
    "                        alpha=0.7,\n",
    "                        label=f\"{label} ({key})\"\n",
    "                    )\n",
    "                # ax.legend(loc='upper right', fontsize=8)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        cmfile = os.path.join(output_dir, f'CM_{label}.png')\n",
    "        plt.savefig(cmfile, bbox_inches='tight', dpi=300)\n",
    "        # plt.suptitle(f\"Class {label} ({idx}) Confusion Matrix Scatter\")\n",
    "        # plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d5a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_read_mask(path):\n",
    "    mask = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if mask is None:\n",
    "        raise FileNotFoundError(f\"Cannot read image: {path}\")\n",
    "    return mask\n",
    "\n",
    "def preprocess_masks(gt_mask, pred_mask):\n",
    "    gt_mask = gt_mask.copy()\n",
    "    pred_mask = pred_mask.copy()\n",
    "    gt_mask[gt_mask == 7] = 0\n",
    "    gt_mask[gt_mask == 13] = 0\n",
    "    pred_mask[pred_mask == 200] = 255\n",
    "    return gt_mask, pred_mask\n",
    "\n",
    "def binarize_labels(gt_mask, pred_mask, class_labels, ignore_labels):\n",
    "    valid_mask = ~np.isin(gt_mask, list(ignore_labels))\n",
    "    gt_flat = gt_mask.flatten()\n",
    "    pred_flat = pred_mask.flatten()\n",
    "    gt_flat[~valid_mask.flatten()] = 0\n",
    "    pred_flat[~valid_mask.flatten()] = 0\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(class_labels)\n",
    "    return lb.transform(gt_flat), lb.transform(pred_flat)\n",
    "\n",
    "def plot_roc(gt_bin, pred_bin, class_labels, class_names, colors, output_path):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i, label in enumerate(class_labels):\n",
    "        fpr, tpr, _ = roc_curve(gt_bin[:, i], pred_bin[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=colors[i], lw=2, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontweight='bold', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontweight='bold', fontsize=14)\n",
    "    ax = plt.gca()\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(12)\n",
    "        tick.label1.set_fontweight('bold')\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(12)\n",
    "        tick.label1.set_fontweight('bold')\n",
    "\n",
    "    # Optionally, thicken the axis lines\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['top'].set_linewidth(2)\n",
    "    ax.spines['right'].set_linewidth(2)\n",
    "    plt.legend(loc='lower right', frameon=True, fontsize=12, prop={'weight': 'bold'}, fancybox=True)\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_pr(gt_bin, pred_bin, class_labels, class_names, colors, output_path):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, label in enumerate(class_labels):\n",
    "        precision, recall, _ = precision_recall_curve(gt_bin[:, i], pred_bin[:, i])\n",
    "        average_precision = average_precision_score(gt_bin[:, i], pred_bin[:, i])\n",
    "        plt.plot(recall, precision, lw=2, color=colors[i], label=f'{class_names[i]} (AP = {average_precision:.2f})')\n",
    "    plt.plot([0, 1], [1, 0], color='gray', linestyle='--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontweight='bold', fontsize=14)\n",
    "    plt.ylabel('Precision', fontweight='bold', fontsize=14)\n",
    "    ax = plt.gca()\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(12)\n",
    "        tick.label1.set_fontweight('bold')\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(12)\n",
    "        tick.label1.set_fontweight('bold')\n",
    "\n",
    "    # Optionally, thicken the axis lines\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['top'].set_linewidth(2)\n",
    "    ax.spines['right'].set_linewidth(2)\n",
    "    plt.legend(loc='lower left', frameon=True, fontsize=12, prop={'weight': 'bold'}, fancybox=True)\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977de615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels =[0, 100, 255]\n",
    "ignore_labels = [200, 150]\n",
    "patch_sizes = [128, 256, 64]\n",
    "gtFile = '/mnt/g/Data/myGT/GT/*'\n",
    "ratios = [0.25, 0.5, '10K', '20K', '50K', '75K', 0.75] \n",
    "predFile_template = '/mnt/d/Results/wd1e_6/{patch_size}/{ratio}/predictions/*'\n",
    "# Main loop\n",
    "for patch_size in patch_sizes:\n",
    "    for ratio in ratios:\n",
    "        gt_files = sorted(glob(gtFile))\n",
    "        pred_files = sorted(glob(predFile_template.format(patch_size=patch_size, ratio=ratio)))\n",
    "        num_files = min(len(gt_files), len(pred_files))\n",
    "        for i in range(3,4):\n",
    "            gt_file = gt_files[i]\n",
    "            pred_file = pred_files[i]\n",
    "            print(f\"Processing {i+1}/{num_files}: {gt_file} vs {pred_file}\")\n",
    "            output_base = f'/mnt/d/Results/wd1e_6/{patch_size}/{ratio}/Metrics/{i+1}/'\n",
    "            os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                gt_mask = safe_read_mask(gt_file)\n",
    "                pred_mask = safe_read_mask(pred_file)\n",
    "                gt_mask, pred_mask = preprocess_masks(gt_mask, pred_mask)\n",
    "                print(f\"GT Labels: {np.unique(gt_mask)} and predLabels: {np.unique(pred_mask)}\")\n",
    "                # plt.figure(figsize=(18, 6))\n",
    "                # plt.subplot(1, 2, 1)\n",
    "                # plt.imshow(gt_mask, cmap='gray')\n",
    "                # plt.axis('off')\n",
    "                # plt.title('Ground Truth')\n",
    "                # plt.subplot(1, 2, 2)\n",
    "                # plt.imshow(pred_mask, cmap='gray')\n",
    "                # plt.axis('off')\n",
    "                # plt.title('Predicted')\n",
    "                # plt.show()\n",
    "                # plt.close()\n",
    "                # Calculate metrics (implement your calculate_metrics function)\n",
    "                metrics = calculate_metrics(gt_mask, pred_mask, class_labels, ignore_labels=ignore_labels, min_size=1000)\n",
    "                with open(os.path.join(output_base, 'metrics_output.txt'), 'w') as f:\n",
    "                    for class_label, class_metrics in metrics.items():\n",
    "                        f.write(f\"Class {class_label}:\\n\")\n",
    "                        for metric_name, value in class_metrics.items():\n",
    "                            f.write(f\"  {metric_name}: {value:.4f}\\n\")\n",
    "                        f.write('\\n')\n",
    "                print(f\"Metrics saved to {output_base}/metrics_output.txt\")\n",
    "                # ROC and PR curves\n",
    "                gt_bin, pred_bin = binarize_labels(gt_mask, pred_mask, class_labels, ignore_labels)\n",
    "                colors = ['blue', 'green', 'red']\n",
    "                class_names = ['Background', 'Gray Matter', 'White Matter']\n",
    "                plot_roc(gt_bin, pred_bin, class_labels, class_names, colors, os.path.join(output_base, 'roc_curve.png'))\n",
    "                print(f\"ROC curve saved to {output_base}/roc_curve.png\")\n",
    "                plot_pr(gt_bin, pred_bin, class_labels, class_names, colors, os.path.join(output_base, 'precision_recall_curve.png'))\n",
    "                print(f\"Precision-Recall curve saved to {output_base}/precision_recall_curve.png\")\n",
    "                for class_label in class_labels:\n",
    "                    plot_boundaries(gt_mask, pred_mask, class_label, ignore_labels=ignore_labels, min_size=1000, output_dir=output_base)\n",
    "                print(f\"Boundaries plotted and saved to {output_base}\")\n",
    "                # plot_per_class_confusion_scatter(gt_mask, pred_mask, class_labels=class_labels, ignore_label=ignore_labels, output_dir=output_base)\n",
    "                # print(f\"Per-class confusion scatter plots saved to {output_base}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {gt_file} and {pred_file}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
